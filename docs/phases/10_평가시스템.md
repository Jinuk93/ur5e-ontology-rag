# Phase 10: 평가 시스템

> **상태**: ✅ 완료
> **도메인**: 운영 레이어 (Operations)
> **목표**: RAG 시스템 품질 측정 및 평가 프레임워크 구축

---

## 1. 개요

RAG 시스템의 검색 품질과 답변 정확도를 객관적으로 측정하기 위한
벤치마크 데이터셋과 평가 지표를 구현하는 단계.

---

## 2. 태스크

| # | 태스크 | 상태 |
|---|--------|------|
| 1 | 벤치마크 데이터셋 설계 | ✅ |
| 2 | 평가 지표 정의 | ✅ |
| 3 | 데이터셋 구축 (QA 쌍) | ✅ |
| 4 | 평가 실행기 구현 | ✅ |
| 5 | 보고서 생성 | ✅ |

---

## 3. 평가 지표

### 3.1 검색 품질 지표

| 지표 | 설명 | 목표 |
|------|------|------|
| **Recall@K** | 상위 K개 결과 중 관련 문서 포함 비율 | ≥ 85% |
| **Precision@K** | 상위 K개 결과 중 관련 문서 비율 | ≥ 70% |
| **MRR** | Mean Reciprocal Rank | ≥ 0.8 |

### 3.2 답변 품질 지표

| 지표 | 설명 | 목표 |
|------|------|------|
| **Accuracy** | 정답과 일치하는 비율 | ≥ 80% |
| **Hallucination Rate** | 문서에 없는 정보 생성 비율 | ≤ 5% |
| **Citation Accuracy** | 출처 정확도 | ≥ 90% |

### 3.3 지표 계산 공식

```
Recall@K = (K개 결과 중 관련 문서 수) / (전체 관련 문서 수)

Precision@K = (K개 결과 중 관련 문서 수) / K

MRR = (1/N) × Σ(1 / rank_i)  (rank_i = 첫 번째 관련 문서의 순위)

Accuracy = (정확한 답변 수) / (전체 질문 수)

Hallucination Rate = (환각 포함 답변 수) / (전체 답변 수)
```

---

## 4. 벤치마크 데이터셋

### 4.1 데이터셋 구성

| 카테고리 | 파일 | 질문 수 | 설명 |
|----------|------|---------|------|
| 에러코드 | `error_code_qa.json` | 15 | 에러코드 원인/해결 |
| 컴포넌트 | `component_qa.json` | 10 | 부품 관련 질문 |
| 일반 | `general_qa.json` | 10 | 일반 기술 질문 |
| 무효 | `invalid_qa.json` | 5 | ABSTAIN 테스트용 |
| **총계** | - | **40** | - |

### 4.2 QA 데이터 형식

```json
{
  "id": "error_001",
  "question": "C154A3 에러의 원인은 무엇인가요?",
  "expected_answer": "Control Box 팬 오작동으로 인한 에러입니다.",
  "expected_entities": ["C154A3", "Control Box"],
  "expected_chunks": ["error_codes_p015_c002", "error_codes_p015_c003"],
  "category": "error_code",
  "difficulty": "easy"
}
```

### 4.3 데이터셋 파일

**error_code_qa.json**:
```json
[
  {
    "id": "error_001",
    "question": "C154A3 에러의 원인은?",
    "expected_answer": "팬 오작동, 케이블 단선, 먼지 축적",
    "expected_chunks": ["error_codes_p015_c002"]
  },
  {
    "id": "error_002",
    "question": "C15402 에러 해결 방법은?",
    "expected_answer": "전압 확인, 전원 케이블 점검",
    "expected_chunks": ["error_codes_p012_c001"]
  }
  // ... 15개
]
```

---

## 5. 구현

### 5.1 평가 지표 모듈

```python
# src/evaluation/metrics.py

from typing import List, Set
from dataclasses import dataclass

@dataclass
class EvaluationMetrics:
    recall_at_5: float
    precision_at_5: float
    mrr: float
    accuracy: float
    hallucination_rate: float
    citation_accuracy: float

def calculate_recall_at_k(
    retrieved: List[str],
    relevant: Set[str],
    k: int = 5
) -> float:
    """Recall@K 계산"""
    retrieved_set = set(retrieved[:k])
    return len(retrieved_set & relevant) / len(relevant) if relevant else 0.0

def calculate_precision_at_k(
    retrieved: List[str],
    relevant: Set[str],
    k: int = 5
) -> float:
    """Precision@K 계산"""
    retrieved_set = set(retrieved[:k])
    return len(retrieved_set & relevant) / k

def calculate_mrr(
    retrieved: List[str],
    relevant: Set[str]
) -> float:
    """MRR 계산"""
    for i, doc_id in enumerate(retrieved):
        if doc_id in relevant:
            return 1.0 / (i + 1)
    return 0.0
```

### 5.2 평가 실행기

```python
# src/evaluation/evaluator.py

from typing import List
from src.evaluation.benchmark import BenchmarkDataset
from src.evaluation.metrics import EvaluationMetrics, calculate_recall_at_k

class Evaluator:
    def __init__(self, rag_service: RAGService):
        self.rag_service = rag_service

    def evaluate(self, dataset: BenchmarkDataset) -> EvaluationReport:
        """전체 평가 실행"""
        results = []

        for qa in dataset.questions:
            # RAG 실행
            response = self.rag_service.query(qa.question)

            # 검색 평가
            retrieved_chunks = [s.chunk_id for s in response.sources]
            recall = calculate_recall_at_k(retrieved_chunks, set(qa.expected_chunks))

            # 답변 평가
            accuracy = self._evaluate_answer(response.answer, qa.expected_answer)
            hallucination = self._detect_hallucination(response.answer, response.sources)

            results.append(EvaluationResult(
                question_id=qa.id,
                recall_at_5=recall,
                accuracy=accuracy,
                hallucination=hallucination
            ))

        return self._aggregate_results(results)

    def _evaluate_answer(self, answer: str, expected: str) -> float:
        """답변 정확도 평가 (키워드 매칭 기반)"""
        expected_keywords = set(expected.lower().split())
        answer_keywords = set(answer.lower().split())
        overlap = expected_keywords & answer_keywords
        return len(overlap) / len(expected_keywords) if expected_keywords else 0.0
```

### 5.3 보고서 생성

```python
# src/evaluation/reporter.py

class EvaluationReporter:
    def generate_report(self, results: EvaluationReport) -> str:
        """평가 보고서 생성"""
        report = f"""
# RAG 시스템 평가 보고서

## 요약
- 평가 일시: {datetime.now()}
- 총 질문 수: {results.total_questions}

## 검색 품질
| 지표 | 값 | 목표 | 상태 |
|------|-----|------|------|
| Recall@5 | {results.recall_at_5:.1%} | ≥85% | {'✅' if results.recall_at_5 >= 0.85 else '❌'} |
| Precision@5 | {results.precision_at_5:.1%} | ≥70% | {'✅' if results.precision_at_5 >= 0.70 else '❌'} |
| MRR | {results.mrr:.3f} | ≥0.8 | {'✅' if results.mrr >= 0.8 else '❌'} |

## 답변 품질
| 지표 | 값 | 목표 | 상태 |
|------|-----|------|------|
| Accuracy | {results.accuracy:.1%} | ≥80% | {'✅' if results.accuracy >= 0.80 else '❌'} |
| Hallucination Rate | {results.hallucination_rate:.1%} | ≤5% | {'✅' if results.hallucination_rate <= 0.05 else '❌'} |
"""
        return report
```

---

## 6. 산출물

### 6.1 파일 목록

| 파일 | 내용 |
|------|------|
| `src/evaluation/benchmark.py` | 벤치마크 데이터셋 로더 |
| `src/evaluation/metrics.py` | 평가 지표 계산 |
| `src/evaluation/evaluator.py` | 평가 실행기 |
| `src/evaluation/reporter.py` | 보고서 생성 |
| `data/benchmark/error_code_qa.json` | 에러코드 QA (15개) |
| `data/benchmark/component_qa.json` | 컴포넌트 QA (10개) |
| `data/benchmark/general_qa.json` | 일반 QA (10개) |
| `data/benchmark/invalid_qa.json` | 무효 QA (5개) |

### 6.2 실행 방법

```bash
# 평가 실행
python scripts/run_evaluation.py

# 특정 카테고리만 평가
python scripts/run_evaluation.py --category error_code

# 보고서 저장
python scripts/run_evaluation.py --output reports/evaluation_report.md
```

---

## 7. 검증 체크리스트

- [x] 벤치마크 데이터셋 40개 QA 구축
- [x] Recall@5, Precision@5, MRR 계산 구현
- [x] Accuracy, Hallucination Rate 계산 구현
- [x] 평가 보고서 자동 생성
- [x] 평가 스크립트 실행 가능

---

## 8. 다음 단계

→ [Phase 11: Entity Linker 개선](11_EntityLinker개선.md)

---

**Phase**: 10 / 19
**마일스톤**: 서비스 완성 (Phase 7-10) ✅
**작성일**: 2026-01-22
