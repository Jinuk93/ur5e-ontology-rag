#!/usr/bin/env python
# ============================================================
# scripts/run_evaluation.py - í‰ê°€ ì‹œìŠ¤í…œ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
# ============================================================
# Phase 10: RAG ì‹œìŠ¤í…œ ìë™ í‰ê°€
#
# ì‚¬ìš©ë²•:
#   # ì „ì²´ í‰ê°€ ì‹¤í–‰
#   python scripts/run_evaluation.py
#
#   # íŠ¹ì • ì¹´í…Œê³ ë¦¬ë§Œ í‰ê°€
#   python scripts/run_evaluation.py --category error_code
#
#   # íŠ¹ì • ë‚œì´ë„ë§Œ í‰ê°€
#   python scripts/run_evaluation.py --difficulty easy
#
#   # ë¹ ë¥¸ í‰ê°€ (ê·œì¹™ ê¸°ë°˜, LLM ì—†ì´)
#   python scripts/run_evaluation.py --fast
#
#   # ë¦¬í¬íŠ¸ë§Œ ìƒì„± (ì´ì „ ê²°ê³¼ ì‚¬ìš©)
#   python scripts/run_evaluation.py --report-only
# ============================================================

import os
import sys
import argparse
import json
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)


def main():
    parser = argparse.ArgumentParser(
        description="UR5e RAG ì‹œìŠ¤í…œ í‰ê°€",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  python scripts/run_evaluation.py                    # ì „ì²´ í‰ê°€
  python scripts/run_evaluation.py --category error_code  # ì—ëŸ¬ì½”ë“œ ì§ˆë¬¸ë§Œ
  python scripts/run_evaluation.py --fast             # ë¹ ë¥¸ í‰ê°€ (LLM ì—†ì´)
  python scripts/run_evaluation.py --report-only      # ë¦¬í¬íŠ¸ë§Œ ìƒì„±
        """,
    )

    parser.add_argument(
        "--category",
        type=str,
        choices=["error_code", "component", "general", "invalid"],
        help="íŠ¹ì • ì¹´í…Œê³ ë¦¬ë§Œ í‰ê°€",
    )

    parser.add_argument(
        "--difficulty",
        type=str,
        choices=["easy", "medium", "hard"],
        help="íŠ¹ì • ë‚œì´ë„ë§Œ í‰ê°€",
    )

    parser.add_argument(
        "--fast",
        action="store_true",
        help="ë¹ ë¥¸ í‰ê°€ ëª¨ë“œ (ê·œì¹™ ê¸°ë°˜, LLM Judge ì‚¬ìš© ì•ˆ í•¨)",
    )

    parser.add_argument(
        "--report-only",
        action="store_true",
        help="ë¦¬í¬íŠ¸ë§Œ ìƒì„± (ìµœê·¼ í‰ê°€ ê²°ê³¼ ì‚¬ìš©)",
    )

    parser.add_argument(
        "--output-dir",
        type=str,
        default="data/evaluation/results",
        help="ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬",
    )

    parser.add_argument(
        "--quiet",
        action="store_true",
        help="ìƒì„¸ ì¶œë ¥ ë¹„í™œì„±í™”",
    )

    args = parser.parse_args()

    print("=" * 60)
    print("UR5e RAG í‰ê°€ ì‹œìŠ¤í…œ")
    print("=" * 60)

    if args.report_only:
        # ìµœê·¼ ê²°ê³¼ë¡œ ë¦¬í¬íŠ¸ë§Œ ìƒì„±
        generate_report_only(args.output_dir)
    else:
        # í‰ê°€ ì‹¤í–‰
        run_evaluation(
            category=args.category,
            difficulty=args.difficulty,
            use_llm_judge=not args.fast,
            output_dir=args.output_dir,
            verbose=not args.quiet,
        )


def run_evaluation(
    category: str = None,
    difficulty: str = None,
    use_llm_judge: bool = True,
    output_dir: str = "data/evaluation/results",
    verbose: bool = True,
):
    """í‰ê°€ ì‹¤í–‰"""
    from src.evaluation.evaluator import Evaluator
    from src.evaluation.report import ReportGenerator

    # í‰ê°€ê¸° ì´ˆê¸°í™”
    if verbose:
        mode = "LLM Judge" if use_llm_judge else "Rule-based (Fast)"
        print(f"\n[*] í‰ê°€ ëª¨ë“œ: {mode}")
        if category:
            print(f"[*] ì¹´í…Œê³ ë¦¬ í•„í„°: {category}")
        if difficulty:
            print(f"[*] ë‚œì´ë„ í•„í„°: {difficulty}")

    evaluator = Evaluator(
        use_llm_judge=use_llm_judge,
        verbose=verbose,
    )

    # ë²¤ì¹˜ë§ˆí¬ í†µê³„ ì¶œë ¥
    stats = evaluator.benchmark.get_statistics()
    if verbose:
        print(f"\n[*] ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹:")
        print(f"  ì´ í•­ëª©: {stats['total']}")
        print(f"  ì¹´í…Œê³ ë¦¬: {stats['by_category']}")
        print(f"  ë‚œì´ë„: {stats['by_difficulty']}")

    # í‰ê°€ ì‹¤í–‰
    print("\n" + "-" * 60)
    print("í‰ê°€ ì‹¤í–‰ ì¤‘...")
    print("-" * 60)

    try:
        summary = evaluator.evaluate_all(
            category=category,
            difficulty=difficulty,
        )
    except Exception as e:
        print(f"\n[ERROR] í‰ê°€ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return

    # ê²°ê³¼ ì €ì¥
    print("\n" + "-" * 60)
    print("ê²°ê³¼ ì €ì¥ ì¤‘...")
    result_path = evaluator.save_results(summary, output_dir)

    # ë¦¬í¬íŠ¸ ìƒì„±
    report_generator = ReportGenerator()
    report_path = "docs/Phase10_í‰ê°€ê²°ê³¼.md"
    report_generator.generate_markdown(summary, report_path)

    # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
    print("\n" + "=" * 60)
    print("í‰ê°€ ì™„ë£Œ!")
    print("=" * 60)
    print(f"\nğŸ“Š ê²°ê³¼ ìš”ì•½:")
    print(f"  í†µê³¼ìœ¨: {summary.pass_rate:.1%} ({summary.passed_items}/{summary.total_items})")
    print(f"  í‰ê·  ì •í™•ë„: {summary.avg_answer_metrics.accuracy:.1%}")
    print(f"  í™˜ê° ë°©ì§€ìœ¨: {(1 - summary.verification_metrics.hallucination_rate):.1%}")
    print(f"  í‰ê·  ì‘ë‹µ ì‹œê°„: {summary.avg_latency_ms:.0f}ms")

    print(f"\nğŸ“ ì €ì¥ëœ íŒŒì¼:")
    print(f"  ê²°ê³¼ JSON: {result_path}")
    print(f"  ë¦¬í¬íŠ¸ MD: {report_path}")

    # ì¹´í…Œê³ ë¦¬ë³„ ìš”ì•½
    print(f"\nğŸ“‹ ì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥:")
    for cat, data in sorted(summary.by_category.items()):
        print(f"  {cat}: {data['pass_rate']:.0%} ({data['passed']}/{data['total']})")

    return summary


def generate_report_only(output_dir: str):
    """ìµœê·¼ ê²°ê³¼ë¡œ ë¦¬í¬íŠ¸ë§Œ ìƒì„±"""
    from src.evaluation.report import ReportGenerator
    from src.evaluation.evaluator import EvaluationSummary
    from src.evaluation.metrics import RetrievalMetrics, AnswerMetrics, VerificationMetrics

    latest_path = Path(output_dir) / "latest.json"

    if not latest_path.exists():
        print(f"[ERROR] ìµœê·¼ í‰ê°€ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {latest_path}")
        print("[INFO] ë¨¼ì € í‰ê°€ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”: python scripts/run_evaluation.py")
        return

    # JSON ë¡œë“œ
    with open(latest_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    # EvaluationSummary ì¬êµ¬ì„±
    summary = EvaluationSummary(
        total_items=data["total_items"],
        passed_items=data["passed_items"],
        pass_rate=data["pass_rate"],
        avg_retrieval_metrics=RetrievalMetrics(**data["avg_retrieval_metrics"]),
        avg_answer_metrics=AnswerMetrics(**data["avg_answer_metrics"]),
        verification_metrics=VerificationMetrics(**data["verification_metrics"]),
        avg_latency_ms=data["avg_latency_ms"],
        by_category=data["by_category"],
        by_difficulty=data["by_difficulty"],
        timestamp=data.get("timestamp", ""),
        version=data.get("version", "unknown"),
    )

    # ë¦¬í¬íŠ¸ ìƒì„±
    report_generator = ReportGenerator()
    report_path = "docs/Phase10_í‰ê°€ê²°ê³¼.md"
    report_generator.generate_markdown(summary, report_path)

    print(f"\n[OK] ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ: {report_path}")


if __name__ == "__main__":
    main()
