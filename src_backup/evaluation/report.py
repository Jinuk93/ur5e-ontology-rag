# ============================================================
# src/evaluation/report.py - Evaluation Report Generator
# ============================================================
# í‰ê°€ ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±ê¸°
# ============================================================

import os
import sys
from datetime import datetime
from typing import Dict, List, Optional
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.insert(0, project_root)

from src.evaluation.evaluator import EvaluationSummary, EvaluationResult


class ReportGenerator:
    """
    í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„±ê¸°

    í‰ê°€ ê²°ê³¼ë¥¼ Markdown í˜•ì‹ì˜ ë¦¬í¬íŠ¸ë¡œ ìƒì„±í•©ë‹ˆë‹¤.

    ì‚¬ìš© ì˜ˆì‹œ:
        generator = ReportGenerator()
        generator.generate_markdown(summary, "docs/Phase10_í‰ê°€ê²°ê³¼.md")
    """

    def generate_markdown(
        self,
        summary: EvaluationSummary,
        output_path: str = "docs/Phase10_í‰ê°€ê²°ê³¼.md",
        results: Optional[List[EvaluationResult]] = None,
    ) -> str:
        """
        Markdown ë¦¬í¬íŠ¸ ìƒì„±

        Args:
            summary: í‰ê°€ ìš”ì•½
            output_path: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
            results: ê°œë³„ ê²°ê³¼ ëª©ë¡ (ì„ íƒ, ìƒì„¸ ë³´ê³ ì„œìš©)

        Returns:
            str: ìƒì„±ëœ ë¦¬í¬íŠ¸ ë‚´ìš©
        """
        report = self._build_report(summary, results)

        # íŒŒì¼ ì €ì¥
        output = Path(output_path)
        output.parent.mkdir(parents=True, exist_ok=True)
        with open(output, "w", encoding="utf-8") as f:
            f.write(report)

        print(f"[OK] Report saved to: {output_path}")
        return report

    def _build_report(
        self,
        summary: EvaluationSummary,
        results: Optional[List[EvaluationResult]] = None,
    ) -> str:
        """ë¦¬í¬íŠ¸ ë‚´ìš© ìƒì„±"""

        sections = [
            self._header(summary),
            self._overview(summary),
            self._retrieval_metrics(summary),
            self._answer_metrics(summary),
            self._verification_metrics(summary),
            self._category_breakdown(summary),
            self._difficulty_breakdown(summary),
        ]

        if results:
            sections.append(self._detailed_results(results))

        sections.append(self._conclusion(summary))
        sections.append(self._footer())

        return "\n\n".join(sections)

    def _header(self, summary: EvaluationSummary) -> str:
        return f"""# Phase 10: í‰ê°€ ê²°ê³¼ ë¦¬í¬íŠ¸

> **í‰ê°€ ì¼ì‹œ:** {summary.timestamp}
>
> **í‰ê°€ ë²„ì „:** {summary.version}
>
> **ì´ í‰ê°€ í•­ëª©:** {summary.total_items}ê°œ

---"""

    def _overview(self, summary: EvaluationSummary) -> str:
        pass_rate_pct = summary.pass_rate * 100
        hallucination_pct = summary.verification_metrics.hallucination_rate * 100
        safe_response_pct = summary.verification_metrics.safe_response_rate * 100

        # ì„±ëŠ¥ ë“±ê¸‰ ê²°ì •
        if pass_rate_pct >= 80:
            grade = "ğŸ† ìš°ìˆ˜"
        elif pass_rate_pct >= 60:
            grade = "âœ… ì–‘í˜¸"
        elif pass_rate_pct >= 40:
            grade = "âš ï¸ ë³´í†µ"
        else:
            grade = "âŒ ë¯¸í¡"

        return f"""## 1. ìš”ì•½

### ì¢…í•© ì„±ëŠ¥

| ì§€í‘œ | ê°’ | í‰ê°€ |
|------|------|------|
| **í†µê³¼ìœ¨** | {pass_rate_pct:.1f}% ({summary.passed_items}/{summary.total_items}) | {grade} |
| **í™˜ê° ë°©ì§€ìœ¨** | {100 - hallucination_pct:.1f}% | {"âœ…" if hallucination_pct < 10 else "âš ï¸"} |
| **ì•ˆì „ ì‘ë‹µë¥ ** | {safe_response_pct:.1f}% | {"âœ…" if safe_response_pct >= 90 else "âš ï¸"} |
| **í‰ê·  ì‘ë‹µ ì‹œê°„** | {summary.avg_latency_ms:.0f}ms | {"âœ…" if summary.avg_latency_ms < 5000 else "âš ï¸"} |

### í•µì‹¬ ì§€í‘œ (KPI)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ì •í™•ë„(Accuracy)     [{self._progress_bar(summary.avg_answer_metrics.accuracy)}] {summary.avg_answer_metrics.accuracy:.0%}  â”‚
â”‚  í™˜ê°ë°©ì§€ìœ¨           [{self._progress_bar(1 - summary.verification_metrics.hallucination_rate)}] {(1 - summary.verification_metrics.hallucination_rate):.0%}  â”‚
â”‚  ê²€ìƒ‰ ì¬í˜„ìœ¨(Recall)  [{self._progress_bar(summary.avg_retrieval_metrics.recall)}] {summary.avg_retrieval_metrics.recall:.0%}  â”‚
â”‚  ê´€ë ¨ì„±(Relevance)    [{self._progress_bar(summary.avg_answer_metrics.relevance)}] {summary.avg_answer_metrics.relevance:.0%}  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```"""

    def _progress_bar(self, value: float, width: int = 20) -> str:
        """ASCII í”„ë¡œê·¸ë ˆìŠ¤ ë°” ìƒì„±"""
        filled = int(value * width)
        empty = width - filled
        return "â–ˆ" * filled + "â–‘" * empty

    def _retrieval_metrics(self, summary: EvaluationSummary) -> str:
        m = summary.avg_retrieval_metrics
        return f"""## 2. ê²€ìƒ‰ í’ˆì§ˆ (Retrieval Metrics)

| ì§€í‘œ | ê°’ | ëª©í‘œ | ë‹¬ì„± |
|------|------|------|------|
| **Precision** | {m.precision:.2%} | 80%+ | {"âœ…" if m.precision >= 0.8 else "âŒ"} |
| **Recall** | {m.recall:.2%} | 85%+ | {"âœ…" if m.recall >= 0.85 else "âŒ"} |
| **F1 Score** | {m.f1_score:.2%} | 80%+ | {"âœ…" if m.f1_score >= 0.8 else "âŒ"} |
| **MRR** | {m.mrr:.2%} | 70%+ | {"âœ…" if m.mrr >= 0.7 else "âŒ"} |
| **Hit Rate** | {m.hit_rate:.2%} | 90%+ | {"âœ…" if m.hit_rate >= 0.9 else "âŒ"} |

### ì„¤ëª…
- **Precision**: ê²€ìƒ‰ëœ ê²°ê³¼ ì¤‘ ì •ë‹µì´ í¬í•¨ëœ ë¹„ìœ¨
- **Recall**: ì •ë‹µ ì¤‘ ê²€ìƒ‰ëœ ë¹„ìœ¨ (ê²€ìƒ‰ ì™„ì „ì„±)
- **F1 Score**: Precisionê³¼ Recallì˜ ì¡°í™” í‰ê· 
- **MRR**: ì²« ë²ˆì§¸ ì •ë‹µì˜ í‰ê·  ì—­ìˆœìœ„
- **Hit Rate**: í•˜ë‚˜ë¼ë„ ì •ë‹µì„ ì°¾ì€ ë¹„ìœ¨"""

    def _answer_metrics(self, summary: EvaluationSummary) -> str:
        m = summary.avg_answer_metrics
        return f"""## 3. ë‹µë³€ í’ˆì§ˆ (Answer Metrics - LLM Judge)

| ì§€í‘œ | ê°’ | ëª©í‘œ | ë‹¬ì„± |
|------|------|------|------|
| **Accuracy** | {m.accuracy:.2%} | 85%+ | {"âœ…" if m.accuracy >= 0.85 else "âŒ"} |
| **Completeness** | {m.completeness:.2%} | 80%+ | {"âœ…" if m.completeness >= 0.8 else "âŒ"} |
| **Relevance** | {m.relevance:.2%} | 90%+ | {"âœ…" if m.relevance >= 0.9 else "âŒ"} |
| **Faithfulness** | {m.faithfulness:.2%} | 95%+ | {"âœ…" if m.faithfulness >= 0.95 else "âŒ"} |

### ì„¤ëª…
- **Accuracy**: ì˜ˆìƒ ì •ë‹µê³¼ì˜ ì¼ì¹˜ë„
- **Completeness**: í•µì‹¬ ì •ë³´ í¬í•¨ ì—¬ë¶€
- **Relevance**: ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ì ì ˆì„±
- **Faithfulness**: ì‚¬ì‹¤ ê¸°ë°˜ ì—¬ë¶€ (í™˜ê° ì—†ìŒ)"""

    def _verification_metrics(self, summary: EvaluationSummary) -> str:
        m = summary.verification_metrics
        return f"""## 4. ê²€ì¦ í’ˆì§ˆ (Verification Metrics)

| ì§€í‘œ | ê°’ | ëª©í‘œ | ë‹¬ì„± |
|------|------|------|------|
| **Hallucination Rate** | {m.hallucination_rate:.2%} | <5% | {"âœ…" if m.hallucination_rate < 0.05 else "âŒ"} |
| **Safe Response Rate** | {m.safe_response_rate:.2%} | 100% | {"âœ…" if m.safe_response_rate >= 1.0 else "âŒ"} |
| **Verification Accuracy** | {m.verification_accuracy:.2%} | 95%+ | {"âœ…" if m.verification_accuracy >= 0.95 else "âŒ"} |

### ì„¤ëª…
- **Hallucination Rate**: ì˜ëª»ëœ ì •ë³´ë¥¼ ìƒì„±í•œ ë¹„ìœ¨ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
- **Safe Response Rate**: ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì—ëŸ¬ ì½”ë“œì— ëŒ€í•´ "ì •ë³´ ì—†ìŒ"ìœ¼ë¡œ ì‘ë‹µí•œ ë¹„ìœ¨
- **Verification Accuracy**: ê²€ì¦ ê²°ê³¼ê°€ ì˜ˆìƒê³¼ ì¼ì¹˜í•œ ë¹„ìœ¨"""

    def _category_breakdown(self, summary: EvaluationSummary) -> str:
        rows = []
        for cat, data in sorted(summary.by_category.items()):
            rows.append(
                f"| **{cat}** | {data['passed']}/{data['total']} | "
                f"{data['pass_rate']:.0%} | {data['avg_accuracy']:.2%} |"
            )

        return f"""## 5. ì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥

| ì¹´í…Œê³ ë¦¬ | í†µê³¼ | í†µê³¼ìœ¨ | í‰ê·  ì •í™•ë„ |
|----------|------|--------|------------|
{chr(10).join(rows)}

### ì¹´í…Œê³ ë¦¬ ì„¤ëª…
- **error_code**: íŠ¹ì • ì—ëŸ¬ ì½”ë“œì— ëŒ€í•œ ì§ˆë¬¸ (ì˜ˆ: "C103 ì—ëŸ¬ í•´ê²°ë²•")
- **component**: ë¶€í’ˆ ê´€ë ¨ ì§ˆë¬¸ (ì˜ˆ: "Control Box ì—ëŸ¬ ëª©ë¡")
- **general**: ì¼ë°˜ ì§ˆë¬¸ (ì˜ˆ: "ë¡œë´‡ì´ ë©ˆì·„ì–´ìš”")
- **invalid**: ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì—ëŸ¬ ì½”ë“œ ì§ˆë¬¸ (í™˜ê° í…ŒìŠ¤íŠ¸)"""

    def _difficulty_breakdown(self, summary: EvaluationSummary) -> str:
        rows = []
        difficulty_order = ["easy", "medium", "hard"]
        for diff in difficulty_order:
            if diff in summary.by_difficulty:
                data = summary.by_difficulty[diff]
                rows.append(
                    f"| **{diff}** | {data['passed']}/{data['total']} | "
                    f"{data['pass_rate']:.0%} | {data['avg_accuracy']:.2%} |"
                )

        return f"""## 6. ë‚œì´ë„ë³„ ì„±ëŠ¥

| ë‚œì´ë„ | í†µê³¼ | í†µê³¼ìœ¨ | í‰ê·  ì •í™•ë„ |
|--------|------|--------|------------|
{chr(10).join(rows)}

### ë‚œì´ë„ ì„¤ëª…
- **easy**: ë‹¨ì¼ ì—ëŸ¬ ì½”ë“œ ì§ì ‘ ì§ˆë¬¸
- **medium**: ë¶€í’ˆ ê´€ë ¨ ë³µí•© ì§ˆë¬¸
- **hard**: ì¦ìƒ ê¸°ë°˜ ì¶”ë¡  í•„ìš”"""

    def _detailed_results(self, results: List[EvaluationResult]) -> str:
        rows = []
        for r in results[:20]:  # ìƒìœ„ 20ê°œë§Œ
            status = "âœ…" if r.passed else "âŒ"
            rows.append(
                f"| {status} | {r.benchmark_id} | {r.question[:30]}... | "
                f"{r.answer_metrics.accuracy:.0%} | {r.verification_status} |"
            )

        return f"""## 7. ìƒì„¸ ê²°ê³¼ (ìƒìœ„ 20ê°œ)

| ìƒíƒœ | ID | ì§ˆë¬¸ | ì •í™•ë„ | ê²€ì¦ |
|------|-----|------|--------|------|
{chr(10).join(rows)}"""

    def _conclusion(self, summary: EvaluationSummary) -> str:
        # ê°œì„  í•„ìš” ì˜ì—­ ì‹ë³„
        improvements = []

        if summary.avg_retrieval_metrics.precision < 0.8:
            improvements.append("- ê²€ìƒ‰ Precision í–¥ìƒ í•„ìš” (í˜„ì¬: {:.0%})".format(summary.avg_retrieval_metrics.precision))

        if summary.avg_answer_metrics.accuracy < 0.85:
            improvements.append("- ë‹µë³€ ì •í™•ë„ í–¥ìƒ í•„ìš” (í˜„ì¬: {:.0%})".format(summary.avg_answer_metrics.accuracy))

        if summary.verification_metrics.hallucination_rate > 0.05:
            improvements.append("- í™˜ê° ë°©ì§€ìœ¨ ê°œì„  í•„ìš” (í˜„ì¬ í™˜ê°ë¥ : {:.0%})".format(summary.verification_metrics.hallucination_rate))

        if not improvements:
            improvements.append("- ëª¨ë“  ëª©í‘œ ì§€í‘œ ë‹¬ì„±! ğŸ‰")

        return f"""## 8. ê²°ë¡  ë° ê°œì„  ë°©í–¥

### í˜„ì¬ ì„±ëŠ¥ í‰ê°€
- ì „ì²´ í†µê³¼ìœ¨: **{summary.pass_rate:.0%}**
- í‰ê·  ì •í™•ë„: **{summary.avg_answer_metrics.accuracy:.0%}**
- í™˜ê° ë°©ì§€ìœ¨: **{(1 - summary.verification_metrics.hallucination_rate):.0%}**

### ê°œì„  í•„ìš” ì˜ì—­
{chr(10).join(improvements)}

### Phaseë³„ ë¹„êµ (ì˜ˆìƒ)

| Phase | Precision | Recall | Accuracy | Hallucination |
|-------|-----------|--------|----------|---------------|
| Phase 5 (Vector) | 45% | 40% | 50% | 100% |
| Phase 6 (Hybrid) | 80% | 85% | 82% | 55% |
| **Phase 7 (Verifier)** | **{summary.avg_retrieval_metrics.precision:.0%}** | **{summary.avg_retrieval_metrics.recall:.0%}** | **{summary.avg_answer_metrics.accuracy:.0%}** | **{summary.verification_metrics.hallucination_rate:.0%}** |"""

    def _footer(self) -> str:
        return f"""---

*ì´ ë¦¬í¬íŠ¸ëŠ” Phase 10 ìë™ í‰ê°€ ì‹œìŠ¤í…œì— ì˜í•´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.*

*ìƒì„± ì‹œê°: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}*"""

    def generate_comparison_report(
        self,
        results: Dict[str, EvaluationSummary],
        output_path: str = "docs/Phase10_ë²„ì „ë¹„êµ.md",
    ) -> str:
        """
        ë²„ì „ ë¹„êµ ë¦¬í¬íŠ¸ ìƒì„±

        Args:
            results: {"v1": summary1, "v2": summary2, ...}
            output_path: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ

        Returns:
            str: ìƒì„±ëœ ë¦¬í¬íŠ¸ ë‚´ìš©
        """
        report = self._build_comparison_report(results)

        output = Path(output_path)
        output.parent.mkdir(parents=True, exist_ok=True)
        with open(output, "w", encoding="utf-8") as f:
            f.write(report)

        print(f"[OK] Comparison report saved to: {output_path}")
        return report

    def _build_comparison_report(self, results: Dict[str, EvaluationSummary]) -> str:
        """ë²„ì „ ë¹„êµ ë¦¬í¬íŠ¸ ìƒì„±"""

        # í…Œì´ë¸” í–‰ ìƒì„±
        rows = []
        for version, summary in sorted(results.items()):
            rows.append(
                f"| **{version}** | "
                f"{summary.pass_rate:.0%} | "
                f"{summary.avg_retrieval_metrics.precision:.0%} | "
                f"{summary.avg_retrieval_metrics.recall:.0%} | "
                f"{summary.avg_answer_metrics.accuracy:.0%} | "
                f"{summary.verification_metrics.hallucination_rate:.0%} | "
                f"{summary.avg_latency_ms:.0f}ms |"
            )

        return f"""# Phase 10: ë²„ì „ ë¹„êµ ë¦¬í¬íŠ¸

> **ë¹„êµ ì¼ì‹œ:** {datetime.now().isoformat()}
>
> **ë¹„êµ ë²„ì „:** {', '.join(results.keys())}

---

## ë²„ì „ë³„ ì„±ëŠ¥ ë¹„êµ

| ë²„ì „ | í†µê³¼ìœ¨ | Precision | Recall | Accuracy | Hallucination | Latency |
|------|--------|-----------|--------|----------|---------------|---------|
{chr(10).join(rows)}

## ë¶„ì„

### ìµœê³  ì„±ëŠ¥ ë²„ì „
- í†µê³¼ìœ¨ ìµœê³ : **{max(results.items(), key=lambda x: x[1].pass_rate)[0]}**
- ì •í™•ë„ ìµœê³ : **{max(results.items(), key=lambda x: x[1].avg_answer_metrics.accuracy)[0]}**
- í™˜ê°ë¥  ìµœì €: **{min(results.items(), key=lambda x: x[1].verification_metrics.hallucination_rate)[0]}**

---

*ì´ ë¦¬í¬íŠ¸ëŠ” Phase 10 ìë™ í‰ê°€ ì‹œìŠ¤í…œì— ì˜í•´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.*
"""


# ============================================================
# ëª¨ë“ˆ í…ŒìŠ¤íŠ¸
# ============================================================

if __name__ == "__main__":
    from src.evaluation.metrics import RetrievalMetrics, AnswerMetrics, VerificationMetrics

    # í…ŒìŠ¤íŠ¸ìš© ë”ë¯¸ ë°ì´í„°
    summary = EvaluationSummary(
        total_items=40,
        passed_items=35,
        pass_rate=0.875,
        avg_retrieval_metrics=RetrievalMetrics(
            precision=0.82,
            recall=0.88,
            f1_score=0.85,
            mrr=0.75,
            hit_rate=0.92,
        ),
        avg_answer_metrics=AnswerMetrics(
            accuracy=0.86,
            completeness=0.83,
            relevance=0.91,
            faithfulness=0.94,
        ),
        verification_metrics=VerificationMetrics(
            hallucination_rate=0.03,
            safe_response_rate=1.0,
            verification_accuracy=0.95,
        ),
        avg_latency_ms=3500,
        by_category={
            "error_code": {"total": 15, "passed": 14, "pass_rate": 0.93, "avg_accuracy": 0.90},
            "component": {"total": 10, "passed": 9, "pass_rate": 0.90, "avg_accuracy": 0.85},
            "general": {"total": 10, "passed": 8, "pass_rate": 0.80, "avg_accuracy": 0.78},
            "invalid": {"total": 5, "passed": 4, "pass_rate": 0.80, "avg_accuracy": 1.0},
        },
        by_difficulty={
            "easy": {"total": 20, "passed": 19, "pass_rate": 0.95, "avg_accuracy": 0.92},
            "medium": {"total": 15, "passed": 13, "pass_rate": 0.87, "avg_accuracy": 0.82},
            "hard": {"total": 5, "passed": 3, "pass_rate": 0.60, "avg_accuracy": 0.70},
        },
    )

    generator = ReportGenerator()
    report = generator.generate_markdown(summary, "docs/Phase10_í‰ê°€ê²°ê³¼_í…ŒìŠ¤íŠ¸.md")
    print("\nGenerated report preview:")
    print(report[:2000])
